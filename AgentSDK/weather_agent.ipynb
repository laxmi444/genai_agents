{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Information Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project implements an intelligent weather assistant that can understand natural language queries about weather conditions in different locations. The agent leverages OpenAI's language models and the Weatherstack API to provide conversational responses about current weather conditions.\n",
    "\n",
    "## Key Components\n",
    "1. WeatherAgent : The core class that handles:\n",
    "   \n",
    "   - Location extraction from natural language queries\n",
    "   - Weather data retrieval from Weatherstack API\n",
    "   - Natural language response generation\n",
    "   - Conversation context management for follow-up questions\n",
    "2. MCP Integration : Implements the Machine Conversation Protocol to:\n",
    "   \n",
    "   - Expose weather functionality as a tool for the agent\n",
    "   - Handle communication between the agent and external services\n",
    "   - Process tool calls and responses\n",
    "3. OpenAI Function Calling : Utilizes OpenAI's function calling capability to:\n",
    "   \n",
    "   - Allow the model to decide when to call the weather tool\n",
    "   - Structure parameters for weather queries\n",
    "   - Process tool results into conversational responses\n",
    "\n",
    "## Features\n",
    "- Natural Language Understanding : Processes queries like \"What's the weather like in New York?\" or \"Will I need an umbrella tomorrow?\"\n",
    "- Context Awareness : Remembers previous locations for follow-up questions like \"How about in London?\"\n",
    "- Comparative Queries : Handles questions about temperature comparisons and future conditions\n",
    "- Conversational Responses : Generates friendly, informative responses rather than just raw data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\omkar\\anaconda3\\lib\\site-packages (0.28.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\omkar\\anaconda3\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: nest_asyncio in c:\\users\\omkar\\anaconda3\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\omkar\\anaconda3\\lib\\site-packages (from openai) (2.26.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\omkar\\anaconda3\\lib\\site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\omkar\\anaconda3\\lib\\site-packages (from openai) (3.9.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\omkar\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\omkar\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2024.8.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\omkar\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\omkar\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (3.7)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\omkar\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\omkar\\anaconda3\\lib\\site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\omkar\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\omkar\\anaconda3\\lib\\site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\omkar\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.9.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\omkar\\anaconda3\\lib\\site-packages (from tqdm->openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai python-dotenv nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "from agents import Agent\n",
    "from agents.mcp import MCPServer  # MCPServer\n",
    "import nest_asyncio # to allow nested event loops (necessary for Jupyter notebooks)\n",
    "\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# set up API keys\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "WEATHER_API_KEY = os.getenv('WEATHER_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Weather Agent Class\n",
    "The WeatherAgent class is a Python implementation that provides weather information for a given location using natural language processing. It combines the OpenAI API for language understanding and the Weatherstack API for retrieving weather data.\n",
    "\n",
    "### Key Components\n",
    "#### Initialization\n",
    "- The class requires API keys for both Weatherstack and OpenAI services\n",
    "- These keys are used to authenticate API requests\n",
    "#### Methods \n",
    "\n",
    "**1. call_openai_api**\n",
    "- Makes requests to OpenAI's API\n",
    "- Sends messages in a specific format required by OpenAI\n",
    "- Returns the generated text response or None if an error occurs\n",
    "- Handles API errors and exceptions gracefully \n",
    "\n",
    "**2. extract_location**\n",
    "- Uses OpenAI to extract location information from a natural language query\n",
    "- Provides specific instructions to the AI to focus only on location extraction\n",
    "- Returns the extracted location name \n",
    "\n",
    "**3. get_weather_data**\n",
    "- Fetches current weather data from Weatherstack API for a specified location\n",
    "- Processes the API response to extract relevant weather information\n",
    "- Returns a structured dictionary with key weather metrics including:\n",
    "  - Location name and country\n",
    "  - Temperature and \"feels like\" temperature\n",
    "  - Humidity and wind speed\n",
    "  - Weather description and cloud cover\n",
    "\n",
    "**4. generate_weather_response**\n",
    "- Creates a natural language description of weather conditions\n",
    "- Uses OpenAI to convert structured weather data into conversational text\n",
    "- Returns a friendly, informative weather description \n",
    "\n",
    "**5. process_weather_query**\n",
    "- Orchestrates the entire weather information process\n",
    "- Extracts location from user query\n",
    "- Retrieves weather data for that location\n",
    "- Generates a natural language response\n",
    "- Handles errors at each step with appropriate fallback messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Weather Agent Core Implementation\n",
    "class WeatherAgent:\n",
    "    def __init__(self, weather_api_key, openai_api_key):\n",
    "        \"\"\"\n",
    "        Initialize the Weather Agent with API keys.\n",
    "        \n",
    "        Args:\n",
    "            weather_api_key (str): API key for weather service\n",
    "            openai_api_key (str): API key for OpenAI\n",
    "        \"\"\"\n",
    "        self.weather_api_key = weather_api_key\n",
    "        self.openai_api_key = openai_api_key\n",
    "        self.conversation_context = {\n",
    "            \"last_location\": None,\n",
    "            \"last_weather_data\": None,\n",
    "            \"query_history\": []\n",
    "        }\n",
    "    \n",
    "    def call_openai_api(self, messages, model=\"gpt-3.5-turbo\"):\n",
    "        \"\"\"\n",
    "        Call OpenAI API directly using requests.\n",
    "        \n",
    "        Args:\n",
    "            messages (list): List of message objects\n",
    "            model (str): Model to use\n",
    "            \n",
    "        Returns:\n",
    "            str: Response content\n",
    "        \"\"\"\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {self.openai_api_key}\"\n",
    "        }\n",
    "        \n",
    "        # Create the payload\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                \"https://api.openai.com/v1/chat/completions\",\n",
    "                headers=headers,\n",
    "                json=payload\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "            else:\n",
    "                print(f\"Error calling OpenAI API: {response.status_code}\")\n",
    "                print(response.text)\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Exception when calling OpenAI API: {e}\")\n",
    "            return None\n",
    "            \n",
    "    def extract_location(self, user_query):\n",
    "        \"\"\"\n",
    "        Extract location from user query using OpenAI.\n",
    "        \n",
    "        Args:\n",
    "            user_query (str): Natural language weather query\n",
    "        \n",
    "        Returns:\n",
    "            str: Extracted location name or None if no location is found\n",
    "        \"\"\"\n",
    "        # Add context from previous queries\n",
    "        context_prompt = \"\"\n",
    "        if self.conversation_context[\"last_location\"]:\n",
    "            context_prompt = f\"Previous location mentioned was {self.conversation_context['last_location']}. \"\n",
    "        \n",
    "        # Create system message with improved instructions\n",
    "        system_content = (\n",
    "            \"You are a location extraction assistant. Extract the specific city or location from the given query. \"\n",
    "            \"If the query doesn't mention a specific location but refers to a previous location, use that previous location. \"\n",
    "            \"If the query is asking a comparative question like 'Is it warmer than yesterday?' or a future question like \"\n",
    "            \"'Will I need an umbrella tomorrow?' without specifying a location, respond with 'USE_PREVIOUS_LOCATION'. \"\n",
    "            \"If no location is mentioned and there's no previous context, respond with 'NO_LOCATION'.\"\n",
    "        )\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"user\", \"content\": f\"{context_prompt}Extract the location from this query: '{user_query}'.\"}\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            location = self.call_openai_api(messages)\n",
    "            \n",
    "            # Handle special responses\n",
    "            if location == \"USE_PREVIOUS_LOCATION\":\n",
    "                return self.conversation_context[\"last_location\"]\n",
    "            elif location == \"NO_LOCATION\":\n",
    "                return None\n",
    "            \n",
    "            # Store the location in context\n",
    "            self.conversation_context[\"last_location\"] = location\n",
    "            return location\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting location: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Weather Agent Response Generation and Query Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Weather Data Retrieval and Response Generation\n",
    "class WeatherAgent(WeatherAgent):\n",
    "    def get_weather_data(self, location):\n",
    "        \"\"\"\n",
    "        Fetch weather data for a given location using Weatherstack API.\n",
    "        \n",
    "        Args:\n",
    "            location (str): City name or location identifier\n",
    "        \n",
    "        Returns:\n",
    "            dict: Processed weather information\n",
    "        \"\"\"\n",
    "        base_url = \"http://api.weatherstack.com/current\"\n",
    "        params = {\n",
    "            'access_key': self.weather_api_key,\n",
    "            'query': location,\n",
    "            'units': 'm'  # Metric units\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(base_url, params=params)\n",
    "            data = response.json()\n",
    "            \n",
    "            # Check for errors in the response\n",
    "            if 'error' in data:\n",
    "                print(f\"Weather API error: {data['error']['info']}\")\n",
    "                return None\n",
    "            \n",
    "            # Extract relevant weather information\n",
    "            weather_info = {\n",
    "                'location': data['location']['name'],\n",
    "                'country': data['location']['country'],\n",
    "                'temperature': data['current']['temperature'],\n",
    "                'feels_like': data['current']['feelslike'],\n",
    "                'humidity': data['current']['humidity'],\n",
    "                'description': data['current']['weather_descriptions'][0] if data['current']['weather_descriptions'] else 'No description available',\n",
    "                'wind_speed': data['current']['wind_speed'],\n",
    "                'cloudiness': data['current']['cloudcover']\n",
    "            }\n",
    "            \n",
    "            # Store the weather data in context\n",
    "            self.conversation_context[\"last_weather_data\"] = weather_info\n",
    "            return weather_info\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching weather data: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def generate_weather_response(self, weather_data, user_query):\n",
    "        \"\"\"\n",
    "        Generate a natural language response for weather data.\n",
    "        \n",
    "        Args:\n",
    "            weather_data (dict): Weather information\n",
    "            user_query (str): Original user query\n",
    "        \n",
    "        Returns:\n",
    "            str: Descriptive weather response\n",
    "        \"\"\"\n",
    "        # Add context for comparative or future queries\n",
    "        context_prompt = \"\"\n",
    "        if \"yesterday\" in user_query.lower() or \"warmer\" in user_query.lower() or \"colder\" in user_query.lower():\n",
    "            context_prompt = \"This is a comparative question about temperature. \"\n",
    "        elif \"tomorrow\" in user_query.lower() or \"umbrella\" in user_query.lower() or \"rain\" in user_query.lower():\n",
    "            context_prompt = \"This is a question about future weather or precipitation. \"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": f\"You are a friendly weather narrator. Create a conversational weather description. {context_prompt}\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Create a friendly, informative weather description for this query: '{user_query}' using this data: {weather_data}\"}\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            response = self.call_openai_api(messages)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating weather response: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def process_weather_query(self, user_query):\n",
    "        \"\"\"\n",
    "        Process a complete weather query from extraction to response.\n",
    "        \n",
    "        Args:\n",
    "            user_query (str): Natural language weather query\n",
    "        \n",
    "        Returns:\n",
    "            str: Comprehensive weather information response\n",
    "        \"\"\"\n",
    "        # Store query in history\n",
    "        self.conversation_context[\"query_history\"].append(user_query)\n",
    "        \n",
    "        # Extract location\n",
    "        location = self.extract_location(user_query)\n",
    "        if not location:\n",
    "            if self.conversation_context[\"last_location\"]:\n",
    "                # Use the last location if available\n",
    "                location = self.conversation_context[\"last_location\"]\n",
    "                print(f\"Using previous location: {location}\")\n",
    "            else:\n",
    "                return \"I need to know which location you're asking about. Could you specify a city or place?\"\n",
    "        \n",
    "        # Get weather data\n",
    "        weather_data = self.get_weather_data(location)\n",
    "        if not weather_data:\n",
    "            return f\"Sorry, I couldn't retrieve weather data for {location}.\"\n",
    "        \n",
    "        # Generate natural language response\n",
    "        weather_response = self.generate_weather_response(weather_data, user_query)\n",
    "        return weather_response or \"I encountered an issue generating the weather description.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. MCP Server Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The WeatherMCPServer class that extends MCPServer to:\n",
    "- Create and manage a WeatherAgent instance\n",
    "- Implement required methods like connect , cleanup , call_tool , and list_tools\n",
    "- Expose the weather functionality as a tool that can be called by the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCP Server Implementation for Weather\n",
    "class WeatherMCPServer(MCPServer):\n",
    "    def __init__(self, weather_agent=None):\n",
    "        super().__init__()\n",
    "        self.weather_agent = weather_agent or WeatherAgent(\n",
    "            weather_api_key=WEATHER_API_KEY, \n",
    "            openai_api_key=OPENAI_API_KEY\n",
    "        )\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        \"\"\"Return the name of the server.\"\"\"\n",
    "        return \"weather_server\"\n",
    "    \n",
    "    async def connect(self):\n",
    "        \"\"\"Connect to the server.\"\"\"\n",
    "        # No actual connection needed for this example\n",
    "        return True\n",
    "    \n",
    "    async def cleanup(self):\n",
    "        \"\"\"Clean up resources.\"\"\"\n",
    "        # No cleanup needed for this example\n",
    "        pass\n",
    "    \n",
    "    async def call_tool(self, tool_name, tool_params):\n",
    "        \"\"\"Call a tool by name with parameters.\"\"\"\n",
    "        if tool_name == \"weather_tool\":\n",
    "            return self.weather_agent.process_weather_query(tool_params[\"query\"])\n",
    "        raise ValueError(f\"Unknown tool: {tool_name}\")\n",
    "    \n",
    "    async def list_tools(self):\n",
    "        \"\"\"List available tools.\"\"\"\n",
    "        # Create a weather tool using our custom FunctionTool\n",
    "        weather_tool = FunctionTool(\n",
    "            name=\"weather_tool\",\n",
    "            description=\"Get current weather information for a location\",\n",
    "            function=lambda params: self.weather_agent.process_weather_query(params[\"query\"]),\n",
    "            parameters={\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The location or weather query to process\"\n",
    "                }\n",
    "            },\n",
    "            required=[\"query\"]\n",
    "        )\n",
    "        return [weather_tool]\n",
    "\n",
    "# Create a persistent instance of the MCP server\n",
    "weather_mcp_server = WeatherMCPServer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Function Tool Implementation\n",
    "\n",
    "A custom FunctionTool class that represents a tool that can be called by the agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FunctionTool:\n",
    "    \"\"\"A simple implementation of FunctionTool\"\"\"\n",
    "    def __init__(self, name, description, function, parameters, required=None):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.function = function\n",
    "        self.parameters = parameters\n",
    "        self.required = required or []\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"FunctionTool(name={self.name})\"\n",
    "    \n",
    "    def to_dict(self):\n",
    "        \"\"\"Convert the tool to a dictionary for easy use\"\"\"\n",
    "        return {\n",
    "            \"name\": self.name,\n",
    "            \"description\": self.description,\n",
    "            \"parameters\": self.parameters,\n",
    "            \"required\": self.required\n",
    "        }\n",
    "\n",
    "# Proper MCP Server implementation for weather\n",
    "class WeatherMCPServer(MCPServer):\n",
    "    def __init__(self, weather_agent=None):\n",
    "        super().__init__()\n",
    "        self.weather_agent = weather_agent or WeatherAgent(\n",
    "            weather_api_key=WEATHER_API_KEY, \n",
    "            openai_api_key=OPENAI_API_KEY\n",
    "        )\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        \"\"\"Return the name of the server.\"\"\"\n",
    "        return \"weather_server\"\n",
    "    \n",
    "    async def connect(self):\n",
    "        \"\"\"Connect to the server.\"\"\"\n",
    "        # No actual connection needed for this example\n",
    "        return True\n",
    "    \n",
    "    async def cleanup(self):\n",
    "        \"\"\"Clean up resources.\"\"\"\n",
    "        # No cleanup needed for this example\n",
    "        pass\n",
    "    \n",
    "    async def call_tool(self, tool_name, tool_params):\n",
    "        \"\"\"Call a tool by name with parameters.\"\"\"\n",
    "        if tool_name == \"weather_tool\":\n",
    "            return self.weather_agent.process_weather_query(tool_params[\"query\"])\n",
    "        raise ValueError(f\"Unknown tool: {tool_name}\")\n",
    "    \n",
    "    async def list_tools(self):\n",
    "        \"\"\"List available tools.\"\"\"\n",
    "        # Create a weather tool using our custom FunctionTool\n",
    "        weather_tool = FunctionTool(\n",
    "            name=\"weather_tool\",\n",
    "            description=\"Get current weather information for a location\",\n",
    "            function=lambda params: self.weather_agent.process_weather_query(params[\"query\"]),\n",
    "            parameters={\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The location or weather query to process\"\n",
    "                }\n",
    "            },\n",
    "            required=[\"query\"]\n",
    "        )\n",
    "        return [weather_tool]\n",
    "\n",
    "# Create a persistent instance of the MCP server\n",
    "weather_mcp_server = WeatherMCPServer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. OpenAI Tools Client and Agent-MCP Integration\n",
    "\n",
    "This cell implements:\n",
    "\n",
    "1. OpenAIToolsClient : A client for the OpenAI API that:\n",
    "   \n",
    "   - Converts MCP tools to the OpenAI function calling format\n",
    "   - Handles calling the OpenAI API with tools\n",
    "   - Processes tool calls and results\n",
    "   \n",
    "2. AgentMCPIntegration : A class that integrates the agent with the MCP server:\n",
    "   \n",
    "   - Creates an agent instance\n",
    "   - Processes messages using the OpenAI client and MCP tools\n",
    "   - Handles the flow of information between the user, agent, and tools\n",
    "This integration allows the agent to use the weather functionality through the OpenAI function calling interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Agent with name 'WeatherAssistant'\n"
     ]
    }
   ],
   "source": [
    "# ppenAI tools client and agent-MCP Integration\n",
    "class OpenAIToolsClient:\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "    \n",
    "    async def get_tools_from_mcp_server(self, server):\n",
    "        \"\"\"Get tools from an MCP server in OpenAI format\"\"\"\n",
    "        tools = await server.list_tools()\n",
    "        openai_tools = []\n",
    "        \n",
    "        for tool in tools:\n",
    "            openai_tool = {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": tool.name,\n",
    "                    \"description\": tool.description,\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": tool.parameters,\n",
    "                        \"required\": tool.required\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            openai_tools.append(openai_tool)\n",
    "        \n",
    "        return openai_tools\n",
    "    \n",
    "    async def call_with_tools(self, messages, tools=None):\n",
    "        \"\"\"Call OpenAI API with tools\"\"\"\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\"\n",
    "        }\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": \"gpt-3.5-turbo\",\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "        \n",
    "        if tools:\n",
    "            payload[\"tools\"] = tools\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                \"https://api.openai.com/v1/chat/completions\",\n",
    "                headers=headers,\n",
    "                json=payload\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            else:\n",
    "                print(f\"Error calling OpenAI API: {response.status_code}\")\n",
    "                print(response.text)\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Exception when calling OpenAI API: {e}\")\n",
    "            return None\n",
    "    \n",
    "    async def process_with_mcp_server(self, messages, server):\n",
    "        \"\"\"Process messages using tools from an MCP server\"\"\"\n",
    "        # Get tools from the MCP server in OpenAI format\n",
    "        tools = await self.get_tools_from_mcp_server(server)\n",
    "        \n",
    "        # Call OpenAI with the tools\n",
    "        response = await self.call_with_tools(messages, tools)\n",
    "        \n",
    "        # If we got a response and it contains a tool call\n",
    "        if response and \"choices\" in response and response[\"choices\"]:\n",
    "            message = response[\"choices\"][0][\"message\"]\n",
    "            \n",
    "            # Check if the message contains tool calls\n",
    "            if \"tool_calls\" in message and message[\"tool_calls\"]:\n",
    "                tool_calls = message[\"tool_calls\"]\n",
    "                \n",
    "                # Process each tool call\n",
    "                for tool_call in tool_calls:\n",
    "                    function_call = tool_call[\"function\"]\n",
    "                    tool_name = function_call[\"name\"]\n",
    "                    tool_params = json.loads(function_call[\"arguments\"])\n",
    "                    \n",
    "                    # Call the tool through the MCP server\n",
    "                    tool_result = await server.call_tool(tool_name, tool_params)\n",
    "                    \n",
    "                    # Add the tool call and result to the messages\n",
    "                    messages.append({\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": None,\n",
    "                        \"tool_calls\": [tool_call]\n",
    "                    })\n",
    "                    \n",
    "                    messages.append({\n",
    "                        \"role\": \"tool\",\n",
    "                        \"tool_call_id\": tool_call[\"id\"],\n",
    "                        \"content\": str(tool_result)\n",
    "                    })\n",
    "                \n",
    "                # Get the final response that incorporates the tool results\n",
    "                final_response = await self.call_with_tools(messages)\n",
    "                \n",
    "                if final_response and \"choices\" in final_response:\n",
    "                    return final_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "            \n",
    "            # If no tool calls, return the content directly\n",
    "            elif \"content\" in message:\n",
    "                return message[\"content\"]\n",
    "        \n",
    "        return \"I'm sorry, I couldn't process your request.\"\n",
    "\n",
    "# Integration class that simulates the Agent-MCP integration\n",
    "class AgentMCPIntegration:\n",
    "    def __init__(self, name, mcp_server, openai_api_key):\n",
    "        \"\"\"\n",
    "        Initialize the Agent-MCP integration\n",
    "        \n",
    "        Args:\n",
    "            name (str): Name of the agent\n",
    "            mcp_server (MCPServer): MCP server to use\n",
    "            openai_api_key (str): OpenAI API key\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.mcp_server = mcp_server\n",
    "        self.openai_api_key = openai_api_key\n",
    "        self.openai_client = OpenAIToolsClient(openai_api_key)\n",
    "        \n",
    "        # Create an Agent instance just for show (we won't use its methods)\n",
    "        try:\n",
    "            self.agent = Agent(name=name)\n",
    "            print(f\"Created Agent with name '{name}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not create Agent: {e}\")\n",
    "            self.agent = None\n",
    "    \n",
    "    async def process_message(self, message):\n",
    "        \"\"\"\n",
    "        Process a message using the MCP server's tools\n",
    "        \n",
    "        Args:\n",
    "            message (str): User message\n",
    "            \n",
    "        Returns:\n",
    "            str: Response\n",
    "        \"\"\"\n",
    "        # Create a messages array with system instructions and user message\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": \"You are a helpful assistant that can answer questions about the weather. Use the weather_tool to get weather information.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": message\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Process the message using our OpenAI client with MCP tools\n",
    "        response = await self.openai_client.process_with_mcp_server(messages, self.mcp_server)\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Create an AgentMCPIntegration instance\n",
    "agent_mcp = AgentMCPIntegration(\n",
    "    name=\"WeatherAssistant\",\n",
    "    mcp_server=weather_mcp_server,\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting weather agent demo...\n",
      "\n",
      "Query: What's the weather like in New York today?\n",
      "Response: The weather in New York today is 13째C with partly cloudy skies. It might feel slightly cooler at 12째C due to the wind blowing at 18 km/h. Enjoy the mix of sun and clouds!\n",
      "\n",
      "Query: How about in London?\n",
      "Response: In London, the current temperature is 12째C with partly cloudy skies and a moderate wind speed of 15 km/h. It's a good day to enjoy some outdoor activities or relax indoors.\n",
      "\n",
      "Query: Is it warmer than yesterday?\n",
      "Response: Yes, it looks like today is slightly warmer than yesterday in London. Today's temperature is 12 degrees Celsius, while yesterday's temperature was also around 12 degrees Celsius. However, it feels just a touch cooler today at 11 degrees with a bit of a breeze.\n",
      "\n",
      "Query: Will I need an umbrella tomorrow?\n",
      "Response: It looks like the weather tomorrow in London, United Kingdom will have partly cloudy skies with a temperature of 12째C. There is no mention of rain, so you should not need an umbrella tomorrow. Enjoy the day!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Process a weather query with the Agent-MCP integration\n",
    "async def process_weather_query(query):\n",
    "    \"\"\"Process a weather query using Agent-MCP integration\"\"\"\n",
    "    return await agent_mcp.process_message(query)\n",
    "\n",
    "# Helper function to run a single query\n",
    "async def run_query(query):\n",
    "    \"\"\"Helper function to run a single query\"\"\"\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    try:\n",
    "        result = await process_weather_query(query)\n",
    "        print(f\"Response: {result}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing query: {e}\")\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Process a series of queries to demonstrate functionality\n",
    "async def run_demo():\n",
    "    \"\"\"Process a series of demo queries\"\"\"\n",
    "    queries = [\n",
    "        \"What's the weather like in New York today?\",\n",
    "        \"How about in London?\",\n",
    "        \"Is it warmer than yesterday?\",\n",
    "        \"Will I need an umbrella tomorrow?\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        await run_query(query)\n",
    "\n",
    "# For Jupyter notebook usage\n",
    "def start_demo():\n",
    "    \"\"\"Start the demo in a way that works in Jupyter notebooks\"\"\"\n",
    "    print(\"Starting weather agent demo...\")\n",
    "    \n",
    "    loop = asyncio.get_event_loop()\n",
    "    try:\n",
    "        loop.run_until_complete(run_demo())\n",
    "    except Exception as e:\n",
    "        print(f\"Demo failed: {e}\")\n",
    "        print(\"\\nTIP: Make sure to run all cells in order before running this demo.\")\n",
    "\n",
    "# When run directly\n",
    "if __name__ == \"__main__\":\n",
    "    start_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
