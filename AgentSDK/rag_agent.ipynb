{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Agents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval-Augmented Generation (RAG) agents combine the power of information retrieval and text generation to create more factual and contextually aware AI responses. Unlike traditional generative models that rely solely on their pre-trained knowledge, RAG systems dynamically retrieve relevant documents from an external knowledge base and incorporate that information into their responses. This process ensures that the AI can provide up-to-date, verifiable, and domain-specific answers rather than relying only on its training data, which may be outdated or limited.\n",
    "\n",
    "The core mechanism of a RAG agent involves two main steps: retrieval and synthesis. First, a retrieval model searches a structured or unstructured knowledge base (such as databases, documents, or APIs) to fetch the most relevant information based on the userâ€™s query. Then, a generative model (e.g., GPT) processes this retrieved data and integrates it into a coherent, context-rich response. This approach is particularly useful in applications like customer support, research assistants, coding helpers, and medical or legal AI advisors, where accuracy and contextual awareness are critical.\n",
    "\n",
    "By incorporating external knowledge sources, retrieval-augmented agents reduce hallucinations, improve response reliability, and adapt to evolving information. They can be fine-tuned to retrieve domain-specific knowledge, making them more effective in specialized fields. Additionally, they offer a practical solution to the limitations of static language models, ensuring that AI systems remain scalable, factually accurate, and continuously improving in their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "from typing import List, Dict, Any, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "import mcp\n",
    "\n",
    "OPENAI_API_KEY = \"your_api_key\"\n",
    "\n",
    "# Load embedding model\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Document store\n",
    "documents = []\n",
    "\n",
    "# faiss setup for parallel vector search\n",
    "dimension = 384  # dimension \n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "faiss.omp_set_num_threads(4)  # Use 4 threads for FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pydantic Models\n",
    "\n",
    "This cell defines structured input schemas using Pydantic. These models provide type validation and clear documentation for the tool interfaces. The models include **DocumentInput** for single document additions, **DocumentsInput** for batch processing, **SearchInput** for configuring searches, and **RagInput** for question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tool schemas using Pydantic\n",
    "class DocumentInput(BaseModel):\n",
    "    document: str = Field(..., description=\"Text document to add to the knowledge base\")\n",
    "\n",
    "class DocumentsInput(BaseModel):\n",
    "    documents: List[str] = Field(..., description=\"List of text documents to add to the knowledge base\")\n",
    "\n",
    "class SearchInput(BaseModel):\n",
    "    query: str = Field(..., description=\"Search query text\")\n",
    "    top_k: int = Field(3, description=\"Number of top results to return\")\n",
    "\n",
    "class RagInput(BaseModel):\n",
    "    question: str = Field(..., description=\"Question to answer using RAG approach\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Core Functions\n",
    "This cell implements the essential RAG functionality. The add_documents function handles parallel embedding processing with batching for efficiency. The search_documents function performs vector similarity search using FAISS. The rag_agent function combines retrieval and generation by fetching relevant documents and using them to inform the LLM's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core functions\n",
    "def add_documents(new_docs):\n",
    "    \"\"\"Add documents with parallel embedding processing\"\"\"\n",
    "    global documents, index\n",
    "    \n",
    "    # Determine the best processing approach based on document count\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        if len(new_docs) > 10:\n",
    "            # For larger sets, split into batches of 32 for parallel processing\n",
    "            batch_size = 32\n",
    "            batches = [new_docs[i:i+batch_size] for i in range(0, len(new_docs), batch_size)]\n",
    "            \n",
    "            # Process batches in parallel\n",
    "            embeddings_list = list(executor.map(embedder.encode, batches))\n",
    "            \n",
    "            # Combine results if multiple batches were processed\n",
    "            if len(embeddings_list) > 1:\n",
    "                embeddings = np.vstack(embeddings_list)\n",
    "            else:\n",
    "                embeddings = embeddings_list[0]\n",
    "        else:\n",
    "            # For smaller sets, process directly\n",
    "            embeddings = embedder.encode(new_docs)\n",
    "    \n",
    "    # Normalize vectors for cosine similarity\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    \n",
    "    # Update the database\n",
    "    documents.extend(new_docs)\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    return len(new_docs), len(documents)\n",
    "\n",
    "def search_documents(query, top_k=3):\n",
    "    \"\"\"Search documents using FAISS for parallel vector similarity\"\"\"\n",
    "    if not documents:\n",
    "        return [\"No documents in the knowledge base.\"]\n",
    "    \n",
    "    # Encode and normalize query\n",
    "    query_embedding = embedder.encode([query])\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "    # Search in FAISS (parallel operation)\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    # Properly check indices and distances\n",
    "    results = []\n",
    "    for i in range(len(indices[0])):\n",
    "        idx = indices[0][i]\n",
    "        # Only include valid indices and positive similarity scores\n",
    "        if idx < len(documents) and distances[0][i] > 0:\n",
    "            results.append(documents[idx])\n",
    "    \n",
    "    return results if results else [\"No relevant documents found.\"]\n",
    "\n",
    "def rag_agent(question):\n",
    "    \"\"\"RAG agent with parallel retrieval and generation\"\"\"\n",
    "    print(\"Retrieving and generating...\")\n",
    "    \n",
    "    # Use ThreadPoolExecutor for parallel operations\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Start retrieval process\n",
    "        retrieval_future = executor.submit(search_documents, question, 3)\n",
    "        \n",
    "        # Get retrieved documents\n",
    "        docs = retrieval_future.result()\n",
    "        context = \"\\n\\n\".join([f\"Document {i+1}: {doc}\" for i, doc in enumerate(docs)])\n",
    "        \n",
    "        # Build system prompt\n",
    "        system_prompt = f\"\"\"You are an intelligent assistant. Use this relevant information:\n",
    "\n",
    "{context}\n",
    "\n",
    "When answering:\n",
    "1. Synthesize information from sources\n",
    "2. Use your own words for a coherent response\n",
    "3. If information is insufficient, acknowledge this\n",
    "4. Never hallucinate information\"\"\"\n",
    "        \n",
    "        # Make API call in parallel thread\n",
    "        api_future = executor.submit(\n",
    "            requests.post,\n",
    "            \"https://api.openai.com/v1/chat/completions\",\n",
    "            headers={\n",
    "                \"Authorization\": f\"Bearer {OPENAI_API_KEY}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            },\n",
    "            json={\n",
    "                \"model\": \"gpt-3.5-turbo\",\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": question}\n",
    "                ],\n",
    "                \"temperature\": 0.3\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Get API response\n",
    "        response = api_future.result()\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            return f\"Error: {response.status_code}, {response.text}\"\n",
    "        \n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. MCP Tools Implementation\n",
    "The MCPRagTools class serves as a standardized interface layer for the RAG system. It transforms the core RAG functionality into a structured, reusable format that's compatible with the Model Context Protocol (MCP).\n",
    "\n",
    "This class uses a static method approach to create stateless tool functions that can be easily registered with MCP or called directly. Each method follows a consistent pattern:\n",
    "\n",
    "1. Input Validation : Each method accepts a strongly-typed Pydantic model parameter, which automatically validates the input data structure and provides clear documentation.\n",
    "2. Core Function Delegation : The methods don't implement the actual RAG functionality themselves. Instead, they delegate to the core functions ( add_documents , search_documents , rag_agent ), passing the appropriate parameters.\n",
    "3. Structured Response : Each method returns a standardized dictionary with consistent fields, making it easier for consumers to process the results programmatically.\n",
    "The class implements four primary tools:\n",
    "\n",
    "- add_document : Adds a single document to the knowledge base, returning the updated document count.\n",
    "- add_documents : Adds multiple documents in batch, leveraging parallel processing for efficiency. Returns both the count of newly added documents and the total document count.\n",
    "- search_documents : Performs semantic search against the knowledge base, returning the most relevant documents along with metadata about the search.\n",
    "- rag_query : The primary end-user tool that combines retrieval and generation to answer questions. It returns both the original question and the generated answer.\n",
    "This architecture separates the interface concerns from implementation details, allowing either to change independently. The static method approach ensures these tools can be used without instantiating the class, making them easier to register with frameworks like MCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MCP tools\n",
    "class MCPRagTools:\n",
    "    @staticmethod\n",
    "    def add_document(input_data: DocumentInput) -> Dict[str, Any]:\n",
    "        \"\"\"Add a single document to the knowledge base with embedding\"\"\"\n",
    "        added, total = add_documents([input_data.document])\n",
    "        return {\"status\": \"success\", \"documents_count\": total}\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_documents(input_data: DocumentsInput) -> Dict[str, Any]:\n",
    "        \"\"\"Add multiple documents with parallel embedding processing\"\"\"\n",
    "        added, total = add_documents(input_data.documents)\n",
    "        return {\n",
    "            \"status\": \"success\", \n",
    "            \"added_count\": added,\n",
    "            \"total_documents\": total\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def search_documents(input_data: SearchInput) -> Dict[str, Any]:\n",
    "        \"\"\"Search documents using FAISS for parallel vector similarity\"\"\"\n",
    "        results = search_documents(input_data.query, input_data.top_k)\n",
    "        return {\n",
    "            \"query\": input_data.query,\n",
    "            \"results\": results,\n",
    "            \"result_count\": len(results)\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def rag_query(input_data: RagInput) -> Dict[str, Any]:\n",
    "        \"\"\"Process a question using Retrieval-Augmented Generation\"\"\"\n",
    "        answer = rag_agent(input_data.question)\n",
    "        return {\n",
    "            \"question\": input_data.question,\n",
    "            \"answer\": answer\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. MCP context creation function\n",
    "The create_mcp_context() function is designed to establish a standardized execution environment for your RAG tools. This function serves as a compatibility layer that handles different MCP implementations.\n",
    "\n",
    "The function begins by attempting to create a new MCP Context object, which serves as a container for registered tools. This context provides a standardized way to execute tools by name with appropriate parameters.\n",
    "\n",
    "The most important aspect of this function is its robust approach to tool registration. Since different MCP implementations might use different methods for registering tools, the function implements a three-tiered fallback strategy:\n",
    "\n",
    "1. First, it attempts to use the context.register() method, which is common in some MCP implementations.\n",
    "2. If that fails with an AttributeError (meaning the method doesn't exist), it falls back to trying context.add_tool() , another common registration pattern.\n",
    "3. If both previous methods fail, it attempts a third approach using context.register_tool() .\n",
    "This defensive programming approach ensures maximum compatibility across different MCP versions and implementations. Each registration attempt registers all four RAG tools:\n",
    "\n",
    "- add_document for single document addition\n",
    "- add_documents for batch document processing\n",
    "- search_documents for knowledge retrieval\n",
    "- rag_query for question answering\n",
    "The function ultimately returns the configured context object, which can then be used to execute the registered tools through a standardized interface. This abstraction layer allows your code to work with different MCP implementations without modification, providing flexibility and future-proofing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MCP context\n",
    "def create_mcp_context():\n",
    "    # Create a new MCP context\n",
    "    context = mcp.Context()\n",
    "    \n",
    "    # Register tools with the context\n",
    "    # Note: The exact registration method may vary depending on your MCP implementation\n",
    "    # Common methods include:\n",
    "    try:\n",
    "        # Method 1: Using register method\n",
    "        context.register(\"add_document\", MCPRagTools.add_document)\n",
    "        context.register(\"add_documents\", MCPRagTools.add_documents)\n",
    "        context.register(\"search_documents\", MCPRagTools.search_documents)\n",
    "        context.register(\"rag_query\", MCPRagTools.rag_query)\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            # Method 2: Using add_tool method\n",
    "            context.add_tool(\"add_document\", MCPRagTools.add_document)\n",
    "            context.add_tool(\"add_documents\", MCPRagTools.add_documents)\n",
    "            context.add_tool(\"search_documents\", MCPRagTools.search_documents)\n",
    "            context.add_tool(\"rag_query\", MCPRagTools.rag_query)\n",
    "        except AttributeError:\n",
    "            # Method 3: Using register_tool method\n",
    "            context.register_tool(\"add_document\", MCPRagTools.add_document)\n",
    "            context.register_tool(\"add_documents\", MCPRagTools.add_documents)\n",
    "            context.register_tool(\"search_documents\", MCPRagTools.search_documents)\n",
    "            context.register_tool(\"rag_query\", MCPRagTools.rag_query)\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 6 documents. Knowledge base now has 6 documents.\n",
      "\n",
      "Question: What are the key components of a RAG system?\n",
      "Retrieving and generating...\n",
      "\n",
      "Answer: The key components of a RAG system include an embedding model, a vector database, a retrieval mechanism, and a text generation model. RAG, which stands for Retrieval-Augmented Generation, utilizes these components to enhance language model responses by incorporating external knowledge retrieved from reliable sources to ground the generated text in factual information. This approach helps mitigate hallucination issues in language models by ensuring that the responses are supported by relevant and accurate external data.\n"
     ]
    }
   ],
   "source": [
    "# Sample documents\n",
    "sample_docs = [\n",
    "    \"OpenAI was founded in December 2015 by Sam Altman, Elon Musk, and others with the mission to ensure that artificial general intelligence benefits all of humanity.\",\n",
    "    \"GPT-4 is a multimodal large language model created by OpenAI in 2023, capable of processing both text and image inputs.\",\n",
    "    \"RAG stands for Retrieval-Augmented Generation, a technique to enhance LLM responses with external knowledge by retrieving relevant information and incorporating it into the generation process.\",\n",
    "    \"Vector databases store embeddings of text which can be searched by similarity using mathematical operations like cosine similarity.\",\n",
    "    \"Retrieval-Augmented Generation (RAG) helps address hallucination problems in language models by grounding responses in factual information from reliable sources.\",\n",
    "    \"The key components of a RAG system include an embedding model, a vector database, a retrieval mechanism, and a text generation model.\"\n",
    "]\n",
    "\n",
    "# Add documents directly\n",
    "added, total = add_documents(sample_docs)\n",
    "print(f\"Added {added} documents. Knowledge base now has {total} documents.\")\n",
    "\n",
    "# Example query\n",
    "question = \"What are the key components of a RAG system?\"\n",
    "print(f\"\\nQuestion: {question}\")\n",
    "\n",
    "# Execute RAG query directly\n",
    "answer = rag_agent(question)\n",
    "print(f\"\\nAnswer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCP execute method not available. Using direct method calls instead.\n",
      "Added 6 documents via direct call.\n",
      "\n",
      "Question: What is RAG used for?\n",
      "Retrieving and generating...\n",
      "\n",
      "Direct Answer: RAG, which stands for Retrieval-Augmented Generation, is a technique used to enhance Large Language Models (LLMs) responses by incorporating external knowledge into the generation process. This is achieved by retrieving relevant information from a database and integrating it into the generated text. The key components of a RAG system include an embedding model, a vector database, a retrieval mechanism, and a text generation model.\n"
     ]
    }
   ],
   "source": [
    "# Simplified MCP Usage Example\n",
    "try:\n",
    "    # Register tools directly with MCP if possible\n",
    "    if hasattr(mcp, 'register_tool'):\n",
    "        mcp.register_tool(\"add_document\", MCPRagTools.add_document)\n",
    "        mcp.register_tool(\"add_documents\", MCPRagTools.add_documents)\n",
    "        mcp.register_tool(\"search_documents\", MCPRagTools.search_documents)\n",
    "        mcp.register_tool(\"rag_query\", MCPRagTools.rag_query)\n",
    "    \n",
    "    # Try to execute with MCP if available\n",
    "    if hasattr(mcp, 'execute'):\n",
    "        # Execute add_documents\n",
    "        result = mcp.execute(\"add_documents\", {\"documents\": sample_docs})\n",
    "        print(f\"Added documents via MCP. Result: {result}\")\n",
    "        \n",
    "        # Execute rag_query\n",
    "        question = \"What is RAG used for?\"\n",
    "        print(f\"\\nQuestion: {question}\")\n",
    "        answer_result = mcp.execute(\"rag_query\", {\"question\": question})\n",
    "        print(f\"\\nMCP Answer: {answer_result.get('answer', answer_result)}\")\n",
    "    else:\n",
    "        print(\"MCP execute method not available. Using direct method calls instead.\")\n",
    "        # Fall back to direct method calls\n",
    "        result = MCPRagTools.add_documents(DocumentsInput(documents=sample_docs))\n",
    "        print(f\"Added {result['added_count']} documents via direct call.\")\n",
    "        \n",
    "        question = \"What is RAG used for?\"\n",
    "        print(f\"\\nQuestion: {question}\")\n",
    "        answer_result = MCPRagTools.rag_query(RagInput(question=question))\n",
    "        print(f\"\\nDirect Answer: {answer_result['answer']}\")\n",
    "except Exception as e:\n",
    "    print(f\"MCP execution failed: {e}\")\n",
    "    print(\"Falling back to direct method calls...\")\n",
    "    \n",
    "    # Fall back to direct method calls\n",
    "    result = MCPRagTools.add_documents(DocumentsInput(documents=sample_docs))\n",
    "    print(f\"Added {result['added_count']} documents via direct call.\")\n",
    "    \n",
    "    question = \"What is RAG used for?\"\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    answer_result = MCPRagTools.rag_query(RagInput(question=question))\n",
    "    print(f\"\\nDirect Answer: {answer_result['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: How does RAG help with hallucination?\n",
      "Retrieving and generating...\n",
      "Answer: Retrieval-Augmented Generation (RAG) is a technique that aids in mitigating hallucination issues in language models by incorporating factual information obtained from reliable sources into the generation process. By grounding responses in this external knowledge, RAG helps ensure that the generated content is more accurate and less likely to produce misleading or false information. This approach enhances the overall reliability and credibility of the language model's outputs, thereby reducing the occurrence of hallucinations in the generated text.\n",
      "\n",
      "Question: What is the role of vector databases in RAG?\n",
      "Retrieving and generating...\n",
      "Answer: In a Retrieval-Augmented Generation (RAG) system, vector databases play a crucial role in storing and organizing the external knowledge or information that is retrieved to enhance the language model's responses. These databases contain vector representations of the retrieved information, allowing the system to efficiently search for and access relevant data during the generation process. By utilizing vector databases, the RAG system can effectively integrate external knowledge into the text generation process, improving the quality and relevance of the generated responses.\n"
     ]
    }
   ],
   "source": [
    "# Additional example queries\n",
    "example_questions = [\n",
    "    \"How does RAG help with hallucination?\",\n",
    "    \"What is the role of vector databases in RAG?\"\n",
    "]\n",
    "\n",
    "for q in example_questions:\n",
    "    print(f\"\\nQuestion: {q}\")\n",
    "    answer = rag_agent(q)\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
