{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Agents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval-Augmented Generation (RAG) agents combine the power of information retrieval and text generation to create more factual and contextually aware AI responses. Unlike traditional generative models that rely solely on their pre-trained knowledge, RAG systems dynamically retrieve relevant documents from an external knowledge base and incorporate that information into their responses. This process ensures that the AI can provide up-to-date, verifiable, and domain-specific answers rather than relying only on its training data, which may be outdated or limited.\n",
    "\n",
    "The core mechanism of a RAG agent involves two main steps: retrieval and synthesis. First, a retrieval model searches a structured or unstructured knowledge base (such as databases, documents, or APIs) to fetch the most relevant information based on the userâ€™s query. Then, a generative model (e.g., GPT) processes this retrieved data and integrates it into a coherent, context-rich response. This approach is particularly useful in applications like customer support, research assistants, coding helpers, and medical or legal AI advisors, where accuracy and contextual awareness are critical.\n",
    "\n",
    "By incorporating external knowledge sources, retrieval-augmented agents reduce hallucinations, improve response reliability, and adapt to evolving information. They can be fine-tuned to retrieve domain-specific knowledge, making them more effective in specialized fields. Additionally, they offer a practical solution to the limitations of static language models, ensuring that AI systems remain scalable, factually accurate, and continuously improving in their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Parallelization?\n",
    "Parallelization refers to enabling multiple agents or tasks to run simultaneously, improving efficiency and responsiveness. This is especially useful in multi-agent systems, reinforcement learning, and AI-powered automation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"your_openai_api_key\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document store\n",
    "documents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faiss setup for parallel vector search\n",
    "dimension = 384  # dimension \n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "faiss.omp_set_num_threads(4)  # Use 4 threads for FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Dynamic Document Addition with Embedding Update\n",
    "This function **add_documents** dynamically updates the knowledge base by adding new documents and computing their embeddings, ensuring efficient retrieval in a retrieval-augmented generation (RAG) system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_documents(new_docs):\n",
    "    \"\"\"Add documents with parallel embedding processing\"\"\"\n",
    "    global documents, index\n",
    "    \n",
    "    # Determine the best processing approach based on document count\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        if len(new_docs) > 10:\n",
    "            # For larger sets, split into batches of 32 for parallel processing\n",
    "            batch_size = 32\n",
    "            batches = [new_docs[i:i+batch_size] for i in range(0, len(new_docs), batch_size)]\n",
    "            \n",
    "            # Process batches in parallel\n",
    "            embeddings_list = list(executor.map(embedder.encode, batches))\n",
    "            \n",
    "            # Combine results if multiple batches were processed\n",
    "            if len(embeddings_list) > 1:\n",
    "                embeddings = np.vstack(embeddings_list)\n",
    "            else:\n",
    "                embeddings = embeddings_list[0]\n",
    "        else:\n",
    "            # For smaller sets, process directly\n",
    "            embeddings = embedder.encode(new_docs)\n",
    "    \n",
    "    # Normalize vectors for cosine similarity\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    \n",
    "    # Update the database\n",
    "    documents.extend(new_docs)\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    print(f\"Added {len(new_docs)} documents. Knowledge base now has {len(documents)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Efficient Document Retrieval Using Semantic Search\n",
    "The **search_documents** function retrieves the most relevant documents from the knowledge base based on semantic similarity. It encodes the query, computes cosine similarities with stored embeddings, and returns the top k most relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_documents(query, top_k=3):\n",
    "    \"\"\"Search documents using FAISS for parallel vector similarity\"\"\"\n",
    "    if not documents:\n",
    "        return [\"No documents in the knowledge base.\"]\n",
    "    \n",
    "    # Encode and normalize query\n",
    "    query_embedding = embedder.encode([query])\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "    # Search in FAISS (parallel operation)\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    # Fix: Properly check indices and distances\n",
    "    results = []\n",
    "    for i in range(len(indices[0])):\n",
    "        idx = indices[0][i]\n",
    "        # Only include valid indices and positive similarity scores\n",
    "        if idx < len(documents) and distances[0][i] > 0:\n",
    "            results.append(documents[idx])\n",
    "    \n",
    "    return results if results else [\"No relevant documents found.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Context-Aware RAG Agent for Intelligent Question Answering\n",
    "The **rag_agent function** is an advanced Retrieval-Augmented Generation (RAG) agent that enhances question answering by combining document retrieval and language model synthesis. It retrieves relevant documents from the knowledge base, integrates them into a structured system prompt, and queries OpenAI's API for a well-informed response. This ensures answers are fact-based, contextually rich, and free from hallucinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_agent(question):\n",
    "    \"\"\"RAG agent with parallel retrieval and generation\"\"\"\n",
    "    print(\"Retrieving and generating...\")\n",
    "    \n",
    "    # Use ThreadPoolExecutor for parallel operations\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Start retrieval process\n",
    "        retrieval_future = executor.submit(search_documents, question, 3)\n",
    "        \n",
    "        # Get retrieved documents\n",
    "        docs = retrieval_future.result()\n",
    "        context = \"\\n\\n\".join([f\"Document {i+1}: {doc}\" for i, doc in enumerate(docs)])\n",
    "        \n",
    "        # Build system prompt\n",
    "        system_prompt = f\"\"\"You are an intelligent assistant. Use this relevant information:\n",
    "\n",
    "{context}\n",
    "\n",
    "When answering:\n",
    "1. Synthesize information from sources\n",
    "2. Use your own words for a coherent response\n",
    "3. If information is insufficient , acknowledge this\n",
    "4. Never hallucinate information\"\"\"\n",
    "        \n",
    "        # Make API call in parallel thread\n",
    "        api_future = executor.submit(\n",
    "            requests.post,\n",
    "            \"https://api.openai.com/v1/chat/completions\",\n",
    "            headers={\n",
    "                \"Authorization\": f\"Bearer {OPENAI_API_KEY}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            },\n",
    "            json={\n",
    "                \"model\": \"gpt-3.5-turbo\",\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": question}\n",
    "                ],\n",
    "                \"temperature\": 0.3\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Get API response\n",
    "        response = api_future.result()\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            return f\"Error: {response.status_code}, {response.text}\"\n",
    "        \n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 6 documents. Knowledge base now has 6 documents.\n",
      "Question: What are the key components of a RAG\n",
      "Retrieving and generating...\n",
      "Answer: The key components of a Retrieval-Augmented Generation (RAG) system include an embedding model, a vector database, a retrieval mechanism, and a text generation model. RAG is a technique that enhances language model responses by incorporating external knowledge retrieved from reliable sources to improve the accuracy and factual grounding of generated text.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Add sample documents\n",
    "    sample_docs = [\n",
    "        \"OpenAI was founded in December 2015 by Sam Altman, Elon Musk, and others with the mission to ensure that artificial general intelligence benefits all of humanity.\",\n",
    "        \"GPT-4 is a multimodal large language model created by OpenAI in 2023, capable of processing both text and image inputs.\",\n",
    "        \"RAG stands for Retrieval-Augmented Generation, a technique to enhance LLM responses with external knowledge by retrieving relevant information and incorporating it into the generation process.\",\n",
    "        \"Vector databases store embeddings of text which can be searched by similarity using mathematical operations like cosine similarity.\",\n",
    "        \"Retrieval-Augmented Generation (RAG) helps address hallucination problems in language models by grounding responses in factual information from reliable sources.\",\n",
    "        \"The key components of a RAG system include an embedding model, a vector database, a retrieval mechanism, and a text generation model.\"\n",
    "    ]\n",
    "    add_documents(sample_docs)\n",
    "    \n",
    "    question = \"What are the key components of a RAG\"\n",
    "    print(f\"Question: {question}\")\n",
    "    answer = rag_agent(question)\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
